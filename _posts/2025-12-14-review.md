---
layout: single
title: "Paper review: Bayesian continual learning and forgetting in neural networks"
date: 2025-12-14
categories: [theory, neuroscience, machine-learning, physics]
tags: [bayesian-learning, continual-learning, epistemic-uncertainty, metaplasticity, hessian, curiosity-log]
permalink: /blog/2025-12-14-curiosity-log/
---

## ðŸ“„ Paper
**Bayesian Continual Learning and Forgetting in Neural Networks**  
Djohan Bonnet *et al.*  
*Nature Communications* (2025)  
arXiv / DOI: https://doi.org/10.1038/s41467-025-64601-w

---

## ðŸ¤” Why I read this paper today

Continual learning always feels like a fundamental tension between **plasticity** and **stability**:

- If the model keeps learning, it forgets.
- If it remembers too well, it stops learning.

What I like about this paper is that it reframes both failures as the *same Bayesian pathology*:

> **Standard Bayesian updating without forgetting inevitably collapses epistemic uncertainty.**

Once uncertainty collapses, the model becomes over-confident, rigid, and brittle â€” even when the data distribution changes.

---

## ðŸ’¡ Core idea in one sentence

> **MESU introduces forgetting directly at the posterior level of a Bayesian neural network, so that uncertainty, plasticity, and memory emerge naturally from variational inference.**

---

## ðŸ§  Truncated posterior instead of infinite memory

Standard Bayesian continual learning accumulates evidence forever:
$
p(\omega \mid D_1,\dots,D_t)
$

MESU instead uses a **finite memory window**:
\[
p(\omega \mid D_{t-N},\dots,D_t)
\]

This single modeling choice already explains a lot:

- Old tasks fade smoothly rather than catastrophically
- The posterior variance does not collapse to zero
- Learning remains possible even after long training

In practice, past datasets are replaced by a *tempered posterior*, which leads to a closed-form **forgetting term** in the variational free energy.

---

## ðŸ”„ Learning vs Forgetting in the free energy

The MESU objective decomposes cleanly into:
- **Learning**: fit the current dataset
- **Forgetting**: gently de-consolidate old synapses

The forgetting term explicitly encourages:
- pulling the mean back toward the prior
- increasing posterior variance to free capacity

This makes forgetting an *active, controlled process*, not a failure mode.

---

## ðŸ§¬ Metaplasticity emerges automatically

Each weight is modeled as a Gaussian:
\[
\omega_i \sim \mathcal N(\mu_i, \sigma_i^2)
\]

The update rule implies:
\[
\Delta \mu_i \propto \sigma_i^2 \, \frac{\partial C}{\partial \mu_i}
\]

So:

- Large epistemic uncertainty â†’ fast learning
- Small epistemic uncertainty â†’ slow learning

Plasticity is no longer a global hyperparameter â€” it is **learned per parameter**.

This is exactly what neuroscientists refer to as *metaplasticity*.

---

## ðŸ§® Connection to Hessian and EWC

One of my favorite aspects of the paper is the theoretical unification:

- Posterior variance scales inversely with the Hessian diagonal
- In the large-window limit, MESU recovers **EWC / Synaptic Intelligence**
- The update resembles a **diagonal Newton step** on the variational free energy

From a Bayesian perspective:
- EWC = Laplace approximation with frozen uncertainty
- MESU = Laplace approximation with *dynamic* uncertainty

This explains why EWC often over-consolidates parameters and loses flexibility.

---

## ðŸŒ Epistemic uncertainty and OOD behavior

Because MESU avoids variance collapse, it preserves epistemic uncertainty even after many tasks.

Empirically:
- In-distribution samples show low epistemic uncertainty
- OOD samples retain high uncertainty
- Deterministic methods fail by construction
- Bayesian methods without forgetting become over-confident

This reinforces a key message:

> **OOD detection requires not just Bayesian modeling, but controlled forgetting.**

---

## ðŸ§  Open questions Iâ€™m thinking about

Some directions that feel very natural to explore next:

- What happens beyond mean-field Gaussian posteriors?
- Can the memory window \(N\) be learned instead of fixed?
- Is there a clean DMFT / field-theoretic interpretation of variance dynamics?
- How does this connect to ergodicity breaking in learning dynamics?

It also feels closely related to ideas from:
- Online Laplace
- Broken ergodicity
- Critical slowing down near task saturation

---

## ðŸ§­ Takeaway

This paper convinced me that:

> **Forgetting is not a bug in Bayesian learning â€” it is a missing term.**

Once forgetting is introduced at the posterior level, many problems in continual learning line up in a surprisingly clean way.

Definitely a paper Iâ€™ll return to.

---
